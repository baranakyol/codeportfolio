{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx    #all the packaes I will import I import here\n",
    "import urllib2\n",
    "import re\n",
    "import matplotlib\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import itertools\n",
    "data = urllib2.urlopen('http://people.sc.fsu.edu/~jburkardt/datasets/sgb/homer.dat')\n",
    "homer=data.read()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_nodes(gfile):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(gfile)  #tokenize to create list that I can iterate through I learned about split a bit too late\n",
    "    A=[]\n",
    "    for word in tokens:\n",
    "        if re.match(r'^[A-Z]{2}$|^\\d[A-Z]$|^[A-Z]\\d$|^\\d{2}$',word):#regex to match what Im searching for\n",
    "            if not word in A:   #avoiding duplicates\n",
    "                A.append(word)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_edges(gfile):\n",
    "    gfile=urllib2.urlopen('http://people.sc.fsu.edu/~jburkardt/datasets/sgb/homer.dat').readlines()[566:674] #cut lines to what Im interested in\n",
    "    returnarray=[]    #had to revisit this part and use readlines for accuracy. Hence the line above.\n",
    "    for line in gfile:  #Iterate through all lines\n",
    "        line=line.strip() #get rid of /n s\n",
    "        \n",
    "        for word in line.split(':')[1].split(';'):  #look once beyond : and iterate through ;\n",
    "            limboarray=list(itertools.combinations(word.split(','),2)) #itertools to get combinations of the lists craeted by the iteration above\n",
    "            #limbo array stores these values \n",
    "            for word in limboarray:   #iterate through limbo array to add these combinations to the actual array cumulatively\n",
    "                if not word in returnarray: #avoiding duplicates. Wasn't necessary I guess\n",
    "                    returnarray.append(word)\n",
    "            \n",
    "    return returnarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(read_nodes(homer))\n",
    "G.add_edges_from(read_edges(homer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def Search2(graph,root,array):\n",
    "    \n",
    "    for node in sorted(graph.neighbors(root)):   \n",
    "        \n",
    "        if not node in array:    # instead of indexing and coloring them each step Im using a global array to store \n",
    "            array.append(node)   # values of the visited arrays. Sorted() function ensures alphabetical order.\n",
    "            Search2(graph,node,array)\n",
    "            \n",
    "    return  array\n",
    "\n",
    "def Search(graph,root):   #Because search was recursive I had to use this to be able to create an initial array Q\n",
    "    Q=[root]              # Q's first item is root so the above Search2 algorithm doesnt add it\n",
    "    return Search2(graph,root,Q)  #Call Search2 function which does DFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ulysses = Search(G, 'OD')\n",
    "print ulysses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def connected_components(graph):\n",
    "    returnarray2=[];ifnotarray=[]  #ifnotarray is where I will store the already visited nodes\n",
    "    \n",
    "    for node in sorted(graph.nodes()):  # Iterate through all nodes of the graph and run DFS\n",
    "        if not node in ifnotarray:      # making sure I don't run DFS on nodes already visited\n",
    "            returnarray2.append(Search(graph,node))    #add the connected components to my array\n",
    "            \n",
    "            for x in Search(graph,node):  #add every item visited by Search to my ifnotarray\n",
    "                ifnotarray.append(x)\n",
    "\n",
    "    return returnarray2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "character_interactions = connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "component_sizes = [len(c) for c in character_interactions]  \n",
    "print \"There are 12 connected components in the Iliad:\", len(component_sizes) == 12\n",
    "print \"The giant component has size 542:\", max(component_sizes) == 542\n",
    "print \"There are 5 isolated characters:\", len([c for c in component_sizes if c == 1]) == 5"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
